---
layout: page
title: Thursday, March 7th
sidebar: true
---

## Daily Outline

* TrafficSpy Submissions
* Evaluating TrafficSpy
* TrafficSpy Code Reviews
* Lightning Talk Reviews -- here @ 3PM with Katrina
* OFA Event

## TrafficSpy Submissions

### Submissions

All project submissions are due here by 9:30AM:

https://github.com/gSchool/submissions/blob/master/projects/traffic_spy.markdown

### Test Coverage

Your test suite should be generating a report in `coverage/index.html` with your test coverage percentage every time the suite is run.

### Code Cleanliness

Pull down the latest code from the project assignment to get access to the sanitation rake tasks:

{% terminal %}
$ git remote add upstream git://github.com/gSchool/traffic_spy.git
$ git pull upstream master
{% endterminal %}

Hopefully there will be no merge conflicts. Then:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:lines
$ bundle exec rake sanitation:methods
{% endterminal %}

Any reported issues will affect your style scoring -- fix them up if you have time!

## Evaluating TrafficSpy

You will peer assess *two different TrafficSpy projects* using the instructions below.

### Evaluation Assignments

#### Session 1 - 10:00-10:45

* Geoffrey Schorkopf reviews Aimee Maher
* Bradley Sheehan reviews Elaine Tai
* Phil Battos reviews Jorge Tellez
* Raphael Weiner reviews Blair Anderson
* Ron Rateau reviews James Denman
* Daniel Mee reviews Logan Sears
* Danny Garcia reviews Shane Rogers
* Jennifer Eliuk reviews John Maddux
* Laura Steadman reviews Kyle Suss
* Kareem Grant reviews Erin Drummond
* Paul Blackwell reviews Christopher Knight
* Josh Mejia reviews Chelsea Komlo

#### Session 2 - 11:00-11:45

* Elaine Tai reviews Geoffrey Schorkopf
* Aimee Maher reviews Bradley Sheehan
* Blair Anderson reviews Phil Battos
* Jorge Tellez reviews Raphael Weiner
* Logan Sears reviews Ron Rateau
* James Denman reviews Daniel Mee
* John Maddux reviews Danny Garcia
* Shane Rogers reviews Jennifer Eliuk
* Chelsea Komlo reviews Paul Blackwell
* Christopher Knight reviews Josh Mejia
* Erin Drummond reviews Laura Steadman
* Kyle Suss reviews Kareem Grant

#### Session 3 - 1:00-1:45

* Ron Rateau reviews Jennifer Eliuk
* Daniel Mee reviews Danny Garcia
* Elaine Tai reviews Josh Mejia
* Aimee Maher reviews Paul Blackwell
* Phil Battos reviews Christopher Knight
* Raphael Weiner reviews Chelsea Komlo
* Geoffrey Schorkopf reviews Jorge Tellez
* Bradley Sheehan reviews Blair Anderson
* Logan Sears reviews Kareem Grant
* James Denman reviews Laura Steadman
* Erin Drummond reviews Shane Rogers
* Kyle Suss reviews John Maddux

#### Session 4 - 2:00-2:45

* Danny Garcia reviews Ron Rateau
* Jennifer Eliuk reviews Daniel Mee
* Paul Blackwell reviews Elaine Tai
* Josh Mejia reviews Aimee Maher
* Chelsea Komlo reviews Phil Battos
* Christopher Knight reviews Raphael Weiner
* Blair Anderson reviews Geoffrey Schorkopf
* Jorge Tellez reviews Bradley Sheehan
* Laura Steadman reviews Logan Sears
* Kareem Grant reviews James Denman
* John Maddux reviews Erin Drummond
* Shane Rogers reviews Kyle Suss

### Starting the Eval

#### As the Reviewee

Find your reviewer and get ready to answer questions, explain your ideas, and debug any small setup issues.

#### As the Reviewer

Find the person you're reviewing. Open http://eval.jumpstartlab.com on your machine and load the first evaluation form.

Then follow the setup procedures below:

### Setup Folders and Code

* Create a folder on your system named `traffic_spy_evaluation_1`
* `cd` into that directory
* Clone the project you're evaluating by substituting the correct URL into this instruction:

{% terminal %}
$ git clone git://github.com/username/traffic_spy.git
{% endterminal %}

* Clone the data generator within the same `traffic_spy_evaluation_1` folder:

{% terminal %}
$ git clone git://github.com/gSchool/traffic_spy-generator.git
{% endterminal %}

* Drop your own DB, setup a database for them:

{% terminal %}
$ dropdb traffic_spy
$ createdb traffic_spy
{% endterminal %}

### Start Their Server

Go into their project directory and:

{% terminal %}
$ cd traffic_spy
$ bundle
$ bundle exec rake db:migrate
$ bundle exec shotgun
{% endterminal %}

### Correctness

#### Setup & Run the Data Generator 

Go into the generator directory, install dependencies, and run the generator:

{% terminal %}
$ cd traffic_spy-generator
$ bundle
$ cucumber
{% endterminal %}

Observe the output from Cucumber for any errors. 

#### Visit the Site

In your browser load up http://127.0.0.1:9393/sources/gSchool 

#### Application Details

Look for information on:

##### Expectation: Most Requested URLs

There was an ambiguity around whether popularity should only count GET requests or all requests.

Observe that the most request pages are either:

1. `/`
2. `/articles`
3. `/signup`

Or:

1. `/signup`
2. `/`
3. `/articles`


##### Expectation: Web Browser Breakdown

Observe that they're able to display the hits ranked by Web Browser.

##### Expectation: OS Breakdown

Observe that they're able to display the hits ranked by OS.

##### Expectation: Screen Resolution

Observe that the most popular Screen Resolutions are:

1. `1366 x 768`
2. `1920 x 1080`
3. `1280 x 1024`

##### Expectation: Average Response Time

Observe that the longest average response times are:

1. `/galleries`
2. `/users`
3. `/articles`

##### Expectation: Page Links

There should be click-able links that take you to the details for each recorded page.

##### Expectation: Event Links

There should be clickable links that take you to the details for each recorded event.

#### Application URL Statistics

Visit http://127.0.0.1:9393/sources/gSchool/urls/articles/2

##### Expectation: Response Times

Observe that all the response times are:

* sorted from greatest to least
* in the range 1000..3000 milliseconds

#### Application Events Index

Visit http://127.0.0.1:9393/sources/gSchool/events

##### Expectation: Event Popularity

Observe that the events are sorted most popular to least popular:

1. `SignUpA`
2. `SignUpB`
3. `galleryBtnA`

##### Expectation: Links to Individual Events

And observe that each event has a link to its event detail page.

#### Application Event Details

Visit http://127.0.0.1:9393/sources/gSchool/events/galleryBtnA

##### Expectation: Hour-by-Hour Events

Observe that requests to this event are grouped by hour.

#### Record Your Findings

Then record your findings in the evaluation form. Consider each

```plain
Correctness
4: All expectations are fulfilled without an error or crash
3: One or two of the above expectations were not met
2: Three, Four, or Five of the expectations were not met
1: More than five expectations were not met
0: Program will not run
```

### Effort - Evaluating Extensions

There are no automated tests for extensions. If the project has extensions, look to their `README` for instructions or dig into their test suite.

Use this data in combination with results from the base expectations tests (above) to determine and record the Effort score.

```plain
Effort
5: Program fulfills all Base Expectations with Campaigns, JSON API, and (Dynamic Display or Authenticated Data)
4: Program fulfills all Base Expectations with Campaigns
3: Program fulfills all Base Expectations
2: Program fulfills Base Expectations except for one or two features.
1: Program fulfills some Base Expectations, but more than two features are broken.
0: Program does not fulfill any of the Base Expectations
```

### Testing - Evaluate Test Coverage

Next you'll need to check out their test coverage from within their project directory:

{% terminal %}
$ bundle exec rspec spec
$ open coverage/index.html
{% endterminal %}

Use the code coverage percentage to determine and record the score.

```plain
Testing
4: Testing suite covers >95% of application code
3: Testing suite covers 90-94% of application code
2: Testing suite covers 80-89% of application code
1: Testing suite covers 50-89% of application code
0: Testing suite covers <50% of application code
```

### Style - Evaluate Code Style

Use the automated rake tasks to evaluate the code style from within the project directory:

{% terminal %}
$ bundle
$ bundle exec rake sanitation:all
{% endterminal %}

Count the number of style infractions and determine/record the score.

```plain
Style
4: Source code consistently uses strong code style including lines under 80 characters, methods under 10 lines of code, and correct indentation.
3: Source code uses good code style, but breaks the above criteria in three or fewer spots
2: Source code uses mixed style, with three to six style breaks
1: Source code is generally messy with six to twelve issues
0: Source code is unacceptable, containing more than twelve style issues
```

### Peer Code Review

Spend the remainder of the evaluation period reading the code you're evaluating. Look for techniques that are particularly interesting. Some areas of focus:

* How did they do with using RSpec syntax? Are the tests nice and small, or giant blocks with lots of expectations? Did they make appropriate use of available techniques like `let`, `before(:each)`, etc?
* Check out their server (probably `lib/traffic_spy/server.rb`). Does it have a consistent level of abstraction? Are the responding blocks for each route small and maintainable, or a ton of lines?
* Were they able to break functionality into small objects, or are there just a few large objects doing everything?

Record these notes and thoughts in the **Highlights** and **Lowlights** sections of the evals.

### Submit the Evaluation

Review all your scores for accuracy, add any overall comments you have, and submit the eval.

### Rinse and Repeat

Each of you will evaluate two projects and be evaluated twice.
